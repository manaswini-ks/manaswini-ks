{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manaswini-ks/manaswini-ks/blob/main/batch_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch APIs\n",
        "\n",
        "The main mode of operation in TurboML is streaming, with continuous updates to different components with fresh data. However, TurboML also supports the good ol' fashioned batch APIs. We've already seen examples of this for feature engineering in the quickstart notebook. In this notebook, we'll focus primarily on batch APIs for ML modelling.\n",
        "\n",
        "To make this more interesting, we'll show how we can still have incremental training on batch data."
      ],
      "id": "0",
      "metadata": {
        "id": "0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the environment and install TurboML's SDK.\n",
        "We use `turboml-installer` to set up the environment for TurboML's SDK."
      ],
      "metadata": {
        "id": "j1K4uuqAiQv2"
      },
      "id": "j1K4uuqAiQv2"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q turboml-installer\n",
        "import turboml_installer ; turboml_installer.install_on_colab()"
      ],
      "execution_count": null,
      "metadata": {
        "id": "itSiuOAgiQv4"
      },
      "outputs": [],
      "id": "itSiuOAgiQv4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kernel should now be restarted with TurboML's SDK installed."
      ],
      "metadata": {
        "id": "g_x4whpviQv8"
      },
      "id": "g_x4whpviQv8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to your TurboML instance\n",
        "\n",
        "Note that you can copy and replace this snippet with one from your TurboML homepage."
      ],
      "metadata": {
        "id": "coXC0oJSiQv9"
      },
      "id": "coXC0oJSiQv9"
    },
    {
      "cell_type": "code",
      "source": [
        "import turboml as tb\n",
        "tb.init(backend_url=https://denim-wasp.api.turboml.online, tb_2tcwVLXSgB2uySQ0nGgc1Cp5tS00QCvy_5cefb341=API_KEY)"
      ],
      "execution_count": null,
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "We'll use our standard `FraudDetection` dataset again, but this time without pushing it to the platform. Interfaces like feature engineering and feature selection work in the exact same ways, just without being linked\n",
        "to a platform-managed dataset."
      ],
      "id": "3",
      "metadata": {
        "id": "3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = tb.datasets.FraudDetectionDatasetFeatures()\n",
        "labels = tb.datasets.FraudDetectionDatasetLabels()\n",
        "\n",
        "transactions_p1 = transactions[:100000]\n",
        "labels_p1 = labels[:100000]\n",
        "\n",
        "transactions_p2 = transactions[100000:]\n",
        "labels_p2 = labels[100000:]"
      ],
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_fields = [\n",
        "    \"transactionAmount\",\n",
        "    \"localHour\",\n",
        "]\n",
        "categorical_fields = [\n",
        "    \"digitalItemCount\",\n",
        "    \"physicalItemCount\",\n",
        "    \"isProxyIP\",\n",
        "]\n",
        "features = transactions_p1.get_model_inputs(\n",
        "    numerical_fields=numerical_fields, categorical_fields=categorical_fields\n",
        ")\n",
        "label = labels_p1.get_model_labels(label_field=\"is_fraud\")"
      ],
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "With the features and label defined, we can train a model in a batch way using the learn method."
      ],
      "id": "6",
      "metadata": {
        "id": "6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tb.HoeffdingTreeClassifier(n_classes=2)"
      ],
      "execution_count": null,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained_100K = model.learn(features, label)"
      ],
      "execution_count": null,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've trained a model on the first 100K rows. Now, to update this model on the remaining data, we can create another batch dataset and call the `learn` method. Note that this time, learn is called on a trained model."
      ],
      "id": "9",
      "metadata": {
        "id": "9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = transactions_p2.get_model_inputs(\n",
        "    numerical_fields=numerical_fields, categorical_fields=categorical_fields\n",
        ")\n",
        "label = labels_p2.get_model_labels(label_field=\"is_fraud\")"
      ],
      "execution_count": null,
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fully_trained = model_trained_100K.learn(features, label)"
      ],
      "execution_count": null,
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "We've seen batch inference on deployed models in the quickstart notebook. We can also perform batch inference on these models using the `predict` method."
      ],
      "id": "12",
      "metadata": {
        "id": "12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model_trained_100K.predict(features)\n",
        "print(metrics.roc_auc_score(labels_p2.df[\"is_fraud\"], outputs[\"score\"]))\n",
        "outputs = model_fully_trained.predict(features)\n",
        "print(metrics.roc_auc_score(labels_p2.df[\"is_fraud\"], outputs[\"score\"]))"
      ],
      "execution_count": null,
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment\n",
        "\n",
        "So far, we've only trained a model. We haven't deployed it yet. Deploying a batch trained model is exactly like any other model deployment, except we'll set the `predict_only` option to be True. This means the model won't be updated automatically."
      ],
      "id": "14",
      "metadata": {
        "id": "14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = transactions.to_online(id=\"transactions10\", load_if_exists=True)\n",
        "labels = labels.to_online(id=\"transaction_labels\", load_if_exists=True)"
      ],
      "execution_count": null,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = transactions.get_model_inputs(\n",
        "    numerical_fields=numerical_fields, categorical_fields=categorical_fields\n",
        ")\n",
        "label = labels.get_model_labels(label_field=\"is_fraud\")"
      ],
      "execution_count": null,
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deployed_model = model_fully_trained.deploy(\n",
        "    name=\"predict_only_model\", input=features, labels=label, predict_only=True\n",
        ")"
      ],
      "execution_count": null,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = deployed_model.get_outputs()\n",
        "outputs[-1]"
      ],
      "execution_count": null,
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "In this notebook, we discussed how to train models in a batch paradigm and deploy them. In a separate notebook we'll cover two different statregies to update models, (i) starting from a batch trained model and using continual learning, (ii) training models incrementally in a batch paradigm and updating the deployment with newer versions."
      ],
      "id": "19",
      "metadata": {
        "id": "19"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}